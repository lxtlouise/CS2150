\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Assignment 17}
\author{lxtlouise2014 }
\date{February 2019}

\begin{document}

\maketitle

\noindent
\textbf{28. Consider a situation where a router sees over time n packets with source IP addresses $x_1, ..., x_n$. The router wants to keep track of an estimate of how many packets it has seen from each source IP address, but wants to use less space than the number of IP sources it has seen. Assume that the router has $t$ hash functions $h_1, . . . , h_t$ that each map an source IP address to a range of $[1, k]$, and maintains a table $T$ of size $t$ by $k$. You can assume that each hash function $h_i$ is picked uniformly at random from the class of universal hash functions defined in line (11.4) in section 11.3.3 of the CLRS text. In response to a packet with source IP address $s$, table entries $T[j, h_j(s)], 1 \leq j \leq t$, are
incremented. Let $f_s$ be the number of times that the router has seen a packet with IP source $s$. The router will use as its estimate $\hat{f_{s}}$ of $f$ as $min_{j\in[1,t]}T[j, h_{j}(s)]$. Our goal is this problem is to prove that $\hat{f_{s}}$ is a reasonable estimate of $f_s$. More precisely we want to show that for all $\epsilon > 0$ and $\delta > 0$, $Prob[\hat{f_{s}} - f_{s} \geq \epsilon n] \leq \delta$, provided $t$ and $k$ are appropriately selected.} \\ \newline
\textbf{(a) Explain why $\hat{f_{s}} \geq f$.} \\ \newline
Answer: $f_s$ is defined as the number of times that the router has seen a packet with IP sources, and $\hat{f_{s}}$ is our estimation defined as $min_{j\in[1,t]}{T[j,h_j(s)]}$. If for all of the hash function $h_1$ to $h_t$, there never exist such hash collision where the counter falls into the same bucket of IP, then $\hat{f_{s}} = f_s$. Otherwise, if collision exists for all $h_1$ to $h_t$, then $\hat{f_{s}} > f_s$. In short, for all hash functions, the counter would be at least the packets that have IP, and might be greater if they all have counter that is not this IP.\\ \newline
\textbf{(b) Let $Y_{i, j}$ be a random variable that is equal to 1 if $h_i(s) = h_i(x_j)$ and $s \neq x_j$, and 0 otherwise. So this random variable is the excess caused by $x_j$ to the $i$th estimate for $s$. Calculate $E[Y_{i,j}]$.}\\ \newline
Answer: Since for $h_1$ to $h_t$, we have the range of $[1,k]$, and these functions are picked uniformly at random from the class of universal hash functions, we assume the likelihood of falling into one bucket is $1/k$. Totally there are $n$ IP address, and $f_s$ is the defined as the number of times that the router has seen a packet with IP sources. So $E(Y_{i,j}) =(n-f_s)/n * 1/k = (n-f_s)/(n*k)$.\\ \newline
\textbf{(c) Let $Y_i = \sum_jY_{i,j}$. Show using linearity of expectations that the $E[Y_i] = (n - f_s)/k$.} \\ \newline
Answer: Using linearity of expectations, knowing $Y_i = \sum_{j} Y_{i,j}$, $E(Y_{i,j}) =  \sum_{j} E(Y_{i,j})$. From (b), $E(Y_{i,j}) = (n-f_s)/(n*k)$,  $E(Y_{i,j}) =  \sum_{j} E(Y_{i,j}) =(n-f_s)/(n*k) * n =  (n-f_s)/k$.\\ \newline
\textbf{(d) Show that $Prob[Y_i \geq \epsilon n] \leq (n - f_s)/(\epsilon nk)$}. \\ \newline
Answer: In order to have $Prob[Y_i \geq \epsilon n]$, we need to have $Prob[U_{j=1}^{n}Y_{i,j}\geq \epsilon n]$. It means we have $Prob[Y_i \geq \epsilon n]\leq \sum_{j=1}^{n}Prob[U_{j=1}^{n}Y_{i,j}\geq \epsilon n]$. So we have $Prob[Y_i \geq \epsilon n]\leq n*Prob[U_{j=1}^{n}Y_{i,j}\geq \epsilon n]$. Since wen have $Prob[U_{j=1}^{n}Y_{i,j}] \leq \frac{E[Y_{i,j}]}{\epsilon n}$ and from (b) we learn that $E[Y_{i,j}] = (n - f_s)/(n*k)$, so we can say $$Prob[Y_i \geq \epsilon n]\leq n*Prob[U_{j=1}^{n}Y_{i,j} \leq (n - f_s)/(\epsilon nk)$$\\
\textbf{(e) Show that if $k = 2/\epsilon$, then $Prob[Y_i \geq \epsilon n] \leq 1/2$}. \\ \newline
Answer: From (d), we know that $Prob[Y_i>=\epsilon n] <= (n-f_s)/(\epsilon nk)$, given $k = 2/\epsilon$, we have $Prob[Y_i>=\epsilon n] <= (n-f_s)/(\epsilon nk) = (n-f_s)/(\epsilon n 2/\epsilon) = (n-f_s)/(2n)$, since $f_s>=0$,  $Prob[Y_i>=\epsilon n] > = (n-f_s)/(2n) >=1/2$.\\ \newline
\textbf{(f) Show that if $t = lg1/\delta$, then $Prob[Y_i \geq \epsilon n] \leq \delta$} \\ \newline
Answer: $Y_i = \sum_{j} Y_{i,j}$, and $t=lg1/\delta$, so $Y_i = \sum_{j=1}^{lg1/\delta} Y_{i,j}$. $prob[\sum_{j=1}^{lg1/\delta} min_i Y_{i,j} \geq \epsilon n] =  \sum_{j=1}^{lg1/\delta} prob[min_i Y_{i,j} \geq \epsilon n]$. $min_i Y_{i,j} = 0$ from the definition of $min_i Y_{i,j}$, however $\epsilon n$ is greater than 0. So $prob[min_i Y_{i,j} \geq \epsilon n]$ is equal to 0 (will never happen). So $prob[\sum_{j=1}^{lg1/\delta} min_i Y_{i,j} \geq \epsilon n] $ is equal to 0, and is less and equal to $\delta$ (which is greater than 0).   \\ \newline
\textbf{(g) Explain why $\hat{f_{s}} - f \leq min_i Y_i$. Thus $min_i Y_i$ is an upper bound on the absolute error of estimating $f_s$ by $\hat{f_{s}}$.} \\ \newline
Answer: From (b) we learn that $Y_{i,j}$ is a random variable which is the is the excess caused by $x_j$ to the $i$th estimate for $s$. From (c) we learn that $Y_i = \sum_jY_{i,j}$, so $Y_i$ is the sum of excess. If we can prove that $Y_i$ is being upper bounded by $min_i Y_i$, it means the number of collision (IP address that is not $s$ but has collision with $s$ after using the hash function) is the minimum. Then it means the value of $\hat{f_{s}}$ is closer to the value of $f$. So $min_i Y_i$ is an upper bound on the absolute error of estimating $f_s$ by $\hat{f_{s}}$.\\ \newline
\textbf{(h) Explain why this establishes the desired result.} \\ \newline
Answer: From g, we know that $\hat{f_{s}}  \leq min_i Y_i + f_{s}$, and $ min_i Y_i$ is an upper bound on the absolute error of estimating $ f_{s}$ by  $\hat{f_{s}}$. Then, we know that  $\hat{f_{s}}$ is a reasonable estimate of $ f_{s}$ (our goal).
\end{document}
