\documentclass{article}
\usepackage[utf8]{inputenc}

\title{Assignment 14}
\title{Assignment 13}
\author{Xiaoting Li (xil139) \\
Ziyu Zhang (ziz41) \\
Deniz Unal (des2014)}
\date{}


\begin{document}

\maketitle

\noindent
\textbf{22. Consider the problem of finding the largest $k$ numbers in sorted order from a list of n numbers (see problem 9-1) in the text. Consider the following algorithm: you consider the numbers one by one, maintaining an auxiliary data structure of the largest $k$ numbers seen to date. We get various algorithms depending on what the auxiliary data structure is and how one searches and updates it. For each of the following variations give the worst-case time complexity as a function of $n$ and $k$. For each of the following variations give the average-case time complexity as a function of $n$ and $k$ under the assumption that each input permutation is equally likely.}\\ \newline
\textbf{(a) The auxiliary data structure is an ordered list and you use linear search starting from the end that contains the largest number} \\ \newline
Answer: Let $S_i$ be the number at position $i$ in the list. Let $X_i$ be an indicator random variable. If $S_i$ is compared with elements in the auxiliary data structure, $X_i$ is 1, else, $X_i$ is 0. Since we use linear search starting from the end that contains the largest number, we have to compare the $S_i$ with elements in the auxiliary data structure every time. There are two parts of the total comparisons. The first part is the sorting the first time we have $k$ elements in the auxiliary data structure. The second part is to compare $S_i$ with numbers in the auxiliary data structure to find the correct position for $S_i$ in the auxiliary data structure. So the total number of comparisons is $E(X) = (1+k)k/2 + \sum_{1}^{n}E(X_i) = (1+k)k/2 + klg(n)$.\\ \newline
\textbf{(b) The auxiliary data structure is an ordered list and you use linear search starting from the end that contains the smallest number} \\ \newline
Answer: Let $S_i$ be the number at position $i$ in the list. Let $X_i$ be an indicator random variable. If $S_i$ is compared with elements in the auxiliary data structure, $X_i$ is 1, else, $X_i$ is 0. There are three parts when we compute the time complexity. The first part is the sorting the first time we have $k$ elements in the auxiliary data structure. The second part is the comparison with the smallest number in the auxiliary data structure. Since we use linear search starting from the end that contains the smallest number, if $S_i$ is smaller than the smallest number in the auxiliary data structure, it means there is no need to compare $S_i$ with numbers in the auxiliary data structure. The third part is when $S_i$ is larger than the smallest number, then we need to compare $S_i$ with numbers in the auxiliary data structure to find the correct position for $S_i$ in the auxiliary data structure. So the total number of comparisons is $E(X) = (1+k)k/2 + (n - k) + \sum_{k+1}^{n}E(X_i)$. Since the probability of $S_i$ to be one of the $k$ largest numbers is $k/i$, so $E(X) = (1+k)k/2 + (n - k) - klg(k) + klg(n)$.\\ \newline
\textbf{(c) The auxiliary data structure is a balanced binary search tree and you use standard log time search, insert and delete operations} \\ \newline
Answer: Let $S_i$ be the number at position $i$ in the list. Let $X_i$ be an indicator random variable. If $S_i$ is compared with elements in the auxiliary data structure, $X_i$ is 1, else, $X_i$ is 0. Since we use linear search starting from the end that contains the largest number, we have to compare the $S_i$ with elements in the auxiliary data structure every time. There are two parts of the total comparisons. The first part is the sorting the first time we have $k$ elements in the auxiliary data structure. The second part is to compare $S_i$ with numbers in the auxiliary data structure to find the correct position for $S_i$ in the auxiliary data structure. Since this is a balanced binary search tree, the first part is $lgk$. So the total number of comparisons is $E(X) = lgk + \sum_{1}^{n}E(X_i) = lgk + nlgk = (n+1)lgk$. \\ \newline
\textbf{(d) The auxiliary data structure is a balanced binary search tree and you use standard log time insert and delete operations, but you start your search from the smallest item in the tree} \\ \newline
Answer: Let $S_i$ be the number at position $i$ in the list. Let $X_i$ be an indicator random variable. If $S_i$ is compared with elements in the auxiliary data structure, $X_i$ is 1, else, $X_i$ is 0. There are three parts when we compute the time complexity. The first part is the sorting the first time we have $k$ elements in the auxiliary data structure. The second part is the comparison with the smallest number in the auxiliary data structure. Since we use linear search starting from the end that contains the smallest number, if $S_i$ is smaller than the smallest number in the auxiliary data structure, it means there is no need to compare $S_i$ with numbers in the auxiliary data structure. The third part is when $S_i$ is larger than the smallest number, then we need to compare $S_i$ with numbers in the auxiliary data structure to find the correct position for $S_i$ in the auxiliary data structure. So the total number of comparisons is $E(X) = lgk + (n - k) + \sum_{k+1}^{n}E(X_i) = lgk + (n - k) + (n - k)lgk = (n + 1 - k)lgk + (n - k)$.\\ \newline
\textbf{23. Assume a router sees a stream of IP packets from two different sources. So the router sees a packet, and then can do some minimal computation, then forwards the packet, sees the next packet, etc. The router is trying to determine the similarity of the destination IP addresses for the two different sources while using very little space. Let $A$ be the collection of destination IP addresses for the first source, and B be the collection of destination IP addresses for the second source. Assume that we have a hash function $h: I -> R$ that maps IP addresses to integers, where the range is sufficiently large that the probability of a collision is negligible. Assume that that the hash function h uniformly distributes the collection $I$ of possible IP addresses over the range $R$, and that each source IP is picked uniformly and independently from the domain of possible IP addresses.} \\ \newline
\textbf{(a) First consider a naive approach. Let a be a random element of $A$ and $b$ a random element $B$. If $A = B$, what is the probability $h(a) = h(b)$? If A and B are disjoint, what is the probability that $h(a) = h(b)$? Calculate the probability that $h(a) = h(b)$ in terms of $|A|$, $|B|$, $|A\cup B|$, and $|A\cap B|$.} \\ \newline
Answer: If the probability of a collision is negligible, the only condition that $h(a) = h(b)$ is when $a = b$. So we need to calculate the probability of picking $a$ from $A$ ($P(a)$) and the probability of picking the same element from $B$ . Since we know IP is picked uniformly and independently and we know $A = B$, we need to calculate P(a) * P(a) which is $1/|A| * 1/|A| = 1/|A|^2$ \\
If A and B are disjoint there is no way that our hash function will generate the same value for elements of $A$ and $B$ (Given the range is sufficient enough). So the probability of $h(a) = h(b)$ is 0. \\ \newline
\textbf{(b) Now we turn to something a bit more sophisticated. Let $h_m(A)$ be the minimum integer k such there is an element $x$ of $A$ where $h(x) = k$. Let $h_m(B)$ be the minimum integer k such there is an element x of B where $h(x) = k$. If $A = B$, what is the probability that $h_m(A) = h_m(B)$? If $A$ and $B$ are disjoint, what is the probability that $h_m(A) = h_m(B)$? Remember that we are assuming that the probability of a collision is negligible.}\\ \newline
Answer: After a random permutation of the set elements, the minimum value is picked. We need to pick the same minimum value assuming there are no collisions similar to the previous problem step. The probability of picking the same minimum value after random permutation is equal to the number of elements that our two sets $A$ and $B$ have in common divided by the union of these two sets. Then, if the two sets are equal, the probability of $h_m(A) = h_m(B) = 1$. If the two sets are disjoint, the probability of $h_m(A) = h_m(B)$ is 0.\\ \newline
\textbf{(c) Calculate the probability that $h_m(A) = h_m(B)$ in terms of $|A|$, $|B|$, $|A\cup B|$, and $|A\cap B|$.}\\ \newline
Answer: Based on our explanation for the previous subproblem the the probability that $h_m(A) = h_m(B)$ is: $|A\cap B| / |A\cup B|$. \\ \newline
\textbf{(d) Explain how the probability that $h_m(A) = h_m(B)$ is an estimate of the similarity of $A$ and $B$.} \\ \newline
Answer:  The probability that $h_m(A) = h_m(B)$ gives us the Jacard index which is a measurement that is used to estimate the similarity between finite sample sets. The closer jacard index is to 1, the more similar these two sets are. \\ \newline
\textbf{(e) Explain how to maintain $h_m(A) = h_m(B)$ using constant space, and constant time per IP packet.} \\ \newline
\textbf{24. Assume you have a source of random bits. So in one time unit, this source will produce one random bit (that is 1 with probability 1/2 independent of other bits). Consider the problem of outputting a random permutation of the integers from 1 to $n$. So each of the n! permutations should be produced with probability exactly $1/n!$.} \\ \newline
\textbf{(a) Give an algorithm to solve this problem and show that the expected time of the algorithm is $O(nlogn)$.This includes both the time that your algorithm takes, plus 1 unit of time for each random bit used.} \\ \newline
Anwer: We can generate a random permutation using a similar way as quicksort. Generating a random permutation is similar to forming a particular linear ordering from 1 to $n$. We denote the final result as $P$. And we divide $P$ into halves, one is $P_1$ and the other one is $P_2$. Since we have a source of random bits(generates one random bit), it is similar to flipping coin. Each time, for example when it generates 1 (similar to flipping a coin and get head), then we put an element in $P_1$. If it generates other bits, we put the element in $P_2$. ($P_1$ is the first half, $P_2$ is the second half). Then we use recursive method to keep repeating the same procedure on $P_1$ and $P_2$. Using this method(similar to quicksort), we can generate a random permutation using $O(nlogn)$.\\ \newline
\textbf{(b) Now assume that there is a limited source of at most $n^2$ random bits. Show that there is no algorithm that can solve the problem using expected time $O(n^2)$.}\\ \newline
Answer: Unlike (a), in this question we have a limited source of at most $n^2$ random bits. It means that each time we have to repeatedly generate a $n^2$ bit number $b$ until we have $b < n^2/2$. Then we decide in which position we need to put the element in the permutation. This process takes $O(n^2)$, so there is no algorithm that can solve the problem using expected time $O(n^2)$. 


\end{document}
